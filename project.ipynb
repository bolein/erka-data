{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Web API for an Interactive Recommender Engine with Apache Spark MLlib using real-world e-commerce data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Vadym Ovcharenko [LinkedIn](https://www.linkedin.com/in/vadymovcharenko/) [GitHub](https://github.com/bolein)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Abstract\n",
    "In this project, I used data from a real e-commerce website to prototype two types of recommendation models. The motivation was to provide an evidance for a large pharmaceutical company that their data can bring value. The data was uset to prototype 2 types of recommendation models. To make a more realistic use-case, a data pipeline was built to enable consistent daily model retraining with new data. Both models were wrapped with REST APIs for access from mobile and web applications. As a result, a pilot version of a web-based recommendation engine provides meaningful suggestions for associated products and personal recommendations. If the customer discovers value in this development, they will release more data for further usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Introduction\n",
    "There are several common types of recommendation systems that are used in e-commerce today\n",
    "\n",
    "**TODO** Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data collection\n",
    "### Data extraction\n",
    "The data is warehoused in the customer’s database and is available using a special bus called E-COM. E-COM is a relatively new technology in the customer's data infrastructure and is actively developed. Because of if this fact, I faced a few problems while working with it. For example, at first, there was no method to dowload the whole collection of transactions. Collaborationg with the E-COM development team, we were able to solve issues quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "E-COM exposes a REST API to interact with it. The API is well [documented](https://docs.google.com/document/d/12qB6IpXknP48yfyHkfkvrCxZ-NvEKq-hMchIRLeuHdc/edit#heading=h.91wpu9x8qw). For this project, I used two endpoints specifically:\n",
    "* [GET /orders](https://docs.google.com/document/d/12qB6IpXknP48yfyHkfkvrCxZ-NvEKq-hMchIRLeuHdc/edit#heading=h.mzszi182k2p7) - Get all orders\n",
    "* [GET /goods](https://docs.google.com/document/d/12qB6IpXknP48yfyHkfkvrCxZ-NvEKq-hMchIRLeuHdc/edit#heading=h.yclivj7eu413) - Get all goods (with filtering options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Both models require transaction data. To perform analysis I had to retrieve the data and store it locally. For this purpose I created the following script that downloaded the data from API page by page. I then converted the data to JSON format and stored it on disk. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "E-COM provides sensitive user data, therefore the firwall blocks request for non-whitelisted hosts. To access the E-COM, one IP was whitelisted for our organization, and it belongs to a server that uses E-COM for other purposes. To be able to access E-COM for this project, I had to set up a VPN server on the whitelisted host. I used [this guide](https://www.digitalocean.com/community/tutorials/how-to-set-up-an-openvpn-server-on-ubuntu-16-04#prerequisites) to set up an OpenVPN server on Ubuntu 16.04. This was my first experience with setting up VPN servers and it turned out to be a relatvely simple procedure. I am confused why VPN service providers charge over 10 dollars per month for a [public VPN](https://nordvpn.com) server with limited number of connections, while you can bootstrap a low-cost machine on [Digital Ocean](https://www.digitalocean.com/) and get a dedicated VPN server with UNLIMITED number of connections for less than 10$/month!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "page_count = 9999999999999 # init with max value\n",
    "params = {'page': 1, 'per-page':100}\n",
    "orders = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, display\n",
    "import base64\n",
    "import sys\n",
    "import time\n",
    "import requests\n",
    "\n",
    "def download_all_orders():\n",
    "    global page_count, orders\n",
    "    auth_token = 'U2l0ZU96OkFWNzREOA=='\n",
    "    headers = {'Authorization': 'Basic {}'.format(auth_token)}\n",
    "    url = 'http://ws.erkapharm.com:8990/ecom/hs/orders?expand=basket'\n",
    "    \n",
    "    while params['page'] < page_count:\n",
    "        started = time.time()\n",
    "        res = requests.get(url, params=params, headers=headers).json()\n",
    "        orders += res['orders']\n",
    "        page_count = res['pageCount']\n",
    "        params['page'] = params['page'] + 1\n",
    "        clear_output(wait=True)\n",
    "        progress = params['page']*1./res['pageCount'] * 100\n",
    "        ellapsed_seconds = time.time() - started\n",
    "        left_seconds = ellapsed_seconds * (page_count - params['page'])\n",
    "        print('progress: {:.2f}% Remaining: {} minutes'.format(progress, int(left_seconds/60)))\n",
    "        \n",
    "    return orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "orders = download_all_orders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99700"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('orders_complete.json', 'w') as outfile:\n",
    "    json.dump(orders, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The downloading process took about 3 hours. To better understand the progress of the task, I calculated the progress rate and ETA. The process took about 3 hours to get about 100,000 records through the API. I fetched 100 records per time. I guess that the process took a long time because of a latency overhead - the VPN server is located in Germany and E-COM instance is in Russia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The output file is only 63.4 MB, but once again, we're using transaction data only from one of the sources (one website). The company has multiple transaction sources, such as other websites and offline stores (around 1500 stores around the country). But for the purposes of this demo project, we only have access to one of the sources, however, one of the most important ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For this project, I chose to use Spark MlLib since it provides a great Python API and uses functional-style operators which are very familiar and very pleasing to me. To test Spark code on my local machine, I used [this guide](https://towardsdatascience.com/how-to-use-pyspark-on-your-computer-9c7180075617) and executed spark code directly in this notebook. From now on, I imagined that I was dealing with a huge amount of data, and used PySpark for all operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Transforming phone numbers\n",
    "As a result of the Data collection step, we have a dataset of well-structured transaction records. However, there are some problems with this data that require additional pre-processing. Since E-COM is a data bus, there data flows from different sources and sometimes does not follow the same conventions. For example, the phone number field, which is thought to be a primary user identifier in transaction data happens in different formats:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## pyspark --conf \"spark.mongodb.output.uri=mongodb://127.0.0.1:27017/cs554.associationRules\" --packages org.mongodb.spark:mongo-spark-connector_2.11:2.3.1\n",
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "rddjson = sc.textFile('orders_complete.json')\n",
    "df = sqlContext.read.json(rddjson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+-----------------+\n",
      "|count(1)|length(clientTel)|          example|\n",
      "+--------+-----------------+-----------------+\n",
      "|    4762|               15|  +7(916)539-6205|\n",
      "|   91516|               17|+7 (905) 545-5709|\n",
      "|    3422|               11|      79153706507|\n",
      "+--------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable('orders')\n",
    "phoneLengths = spark.sql('SELECT COUNT(*), LENGTH(clientTel), first(clientTel) as example FROM orders GROUP BY LENGTH(clientTel)')\n",
    "phoneLengths.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We see that there's at least 3 types of different phone formats (depending on the string length). I removed all non-digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+\n",
      "|        clientTel|clientTelNumeric|\n",
      "+-----------------+----------------+\n",
      "|+7 (905) 545-5709|     79055455709|\n",
      "|+7 (915) 182-9688|     79151829688|\n",
      "|+7 (967) 135-7431|     79671357431|\n",
      "|+7 (123) 456-7890|     71234567890|\n",
      "|+7 (123) 456-7890|     71234567890|\n",
      "|+7 (916) 714-9823|     79167149823|\n",
      "|+7 (915) 215-0103|     79152150103|\n",
      "|+7 (926) 172-6883|     79261726883|\n",
      "|+7 (967) 170-4118|     79671704118|\n",
      "|+7 (966) 192-6058|     79661926058|\n",
      "+-----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "strToNum = udf(lambda phone: ''.join(filter(str.isdigit, phone)))\n",
    "df = df.withColumn('clientTelNumeric', strToNum(df.clientTel))\n",
    "df.select('clientTel', 'clientTelNumeric').limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fitlering out duplicate transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There are also some duplicate records in the data. Duplicate transactions can confuse frequent pattern mining algorithms and collaborative filtering models as they will assume that every transaction record is unique. Consequenty, duplicates will artificially increase the frequency of certain datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.686037575585981"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropDuplicates(['id']).cache()\n",
    "df.select('clientTelNumeric').distinct().count()/df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "After parsing only numeric values from the `clientTel` field and dropping duplicate records, we can see that our transaction database has more than **30% of returning customers**. It means that the dataset is a decent case for building collaborative recomendation models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model building\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Building association rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "I decided to start with [Frequent Pattern Mining](https://spark.apache.org/docs/2.3.2/mllib-frequent-pattern-mining.html). I used the FP-growth algorithm, since I studied it in Data Mining class and familiar with how the algorithm works. It turned out, that there's not much information on the Web on how to implement a piece of software using Spark MLlib that will provide association rules as an output of the algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To create an FP-Growth model, we need to extract product IDs from transaction records. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "# explode items, distinct them and aggregate back (to remove duplicate items in transactions)\n",
    "transactions = df.select('id', explode('basket.goodsId')) \\\n",
    "                    .groupby('id') \\\n",
    "                    .agg(collect_set('col').alias('items')) \\\n",
    "                    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "fp = FPGrowth(minSupport=0.0005, minConfidence=0, numPartitions=4)\n",
    "fpm = fp.fit(transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Mining association rules takes quite some time. With the `minSupport` property, we can filter out infrequent itemsets. Depending on the implementation of the recommendation engine, it might make sense to mine as many association rules as possible by keeping the `minSupport` as low as possible. If a website has a \"frequently bought together\" section, it should be filled with items. However, if we want to create a pop-up sreen after click on the \"add to cart\" with a recommended item, we should have a high confidence in such recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Confidence is a probability of the consequent given the antecedent (e.g. how likely is the user to purchase consequent in case he already purchased antecedent). Since the dataset is relatively small, we use a low confidence threshold to generate association rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------------+\n",
      "|antecedent|consequent|         confidence|\n",
      "+----------+----------+-------------------+\n",
      "|   [81976]|   [81977]| 0.5106382978723404|\n",
      "|  [121286]|  [103180]|0.41626794258373206|\n",
      "|  [103180]|  [121286]| 0.3096085409252669|\n",
      "|  [114265]|  [114892]|0.27807486631016043|\n",
      "|  [121563]|   [97807]| 0.2742616033755274|\n",
      "+----------+----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpm.associationRules.sort(desc('confidence')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Validating results\n",
    "We generated a table of association rules that can be used by our Web service to give product recommendations. I decided to fetch product names to intuitively evaluate the results of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "import requests\n",
    "\n",
    "def fetchProductNames(ids):\n",
    "    names = []\n",
    "    for id in ids:\n",
    "        url = 'http://138.68.86.83/api/goods/{}?region=77'.format(id)\n",
    "        res = requests.get(url).json()\n",
    "        names.append(res['name'])\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedent</th>\n",
       "      <th>consequent</th>\n",
       "      <th>confidence</th>\n",
       "      <th>antecedentName</th>\n",
       "      <th>consequentName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[81976]</td>\n",
       "      <td>[81977]</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>[Бриллиантовый зеленый р-р спирт 1% 5мл фломас...</td>\n",
       "      <td>[Йода р-р 5% 5мл фломастер]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[121286]</td>\n",
       "      <td>[103180]</td>\n",
       "      <td>0.416268</td>\n",
       "      <td>[PL Контейнер д/биопроб стер. 60мл с крыш и ло...</td>\n",
       "      <td>[PL Контейнер д/биопроб универс. 120мл полим. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[103180]</td>\n",
       "      <td>[121286]</td>\n",
       "      <td>0.309609</td>\n",
       "      <td>[PL Контейнер д/биопроб универс. 120мл полим. ...</td>\n",
       "      <td>[PL Контейнер д/биопроб стер. 60мл с крыш и ло...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[114265]</td>\n",
       "      <td>[114892]</td>\n",
       "      <td>0.278075</td>\n",
       "      <td>[Нью Лайф Бинт марл мед стер 7м х 14см уп N1/СТМ]</td>\n",
       "      <td>[Нью Лайф Бинт марл мед стер 5м х 10см уп N1/СТМ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[121563]</td>\n",
       "      <td>[97807]</td>\n",
       "      <td>0.274262</td>\n",
       "      <td>[Ля Рош Позе Эфаклар Гель для умывания очищающ...</td>\n",
       "      <td>[Ля Рош-Позе Эфаклар Дуо+ Крем-гель корректиру...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  antecedent consequent  confidence  \\\n",
       "0    [81976]    [81977]    0.510638   \n",
       "1   [121286]   [103180]    0.416268   \n",
       "2   [103180]   [121286]    0.309609   \n",
       "3   [114265]   [114892]    0.278075   \n",
       "4   [121563]    [97807]    0.274262   \n",
       "\n",
       "                                      antecedentName  \\\n",
       "0  [Бриллиантовый зеленый р-р спирт 1% 5мл фломас...   \n",
       "1  [PL Контейнер д/биопроб стер. 60мл с крыш и ло...   \n",
       "2  [PL Контейнер д/биопроб универс. 120мл полим. ...   \n",
       "3  [Нью Лайф Бинт марл мед стер 7м х 14см уп N1/СТМ]   \n",
       "4  [Ля Рош Позе Эфаклар Гель для умывания очищающ...   \n",
       "\n",
       "                                      consequentName  \n",
       "0                        [Йода р-р 5% 5мл фломастер]  \n",
       "1  [PL Контейнер д/биопроб универс. 120мл полим. ...  \n",
       "2  [PL Контейнер д/биопроб стер. 60мл с крыш и ло...  \n",
       "3  [Нью Лайф Бинт марл мед стер 5м х 10см уп N1/СТМ]  \n",
       "4  [Ля Рош-Позе Эфаклар Дуо+ Крем-гель корректиру...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandasRules = rules.limit(5).toPandas()\n",
    "pandasRules['antecedentName'] = pandasRules.antecedent.map(fetchProductNames)\n",
    "pandasRules['consequentName'] = pandasRules.consequent.map(fetchProductNames)\n",
    "pandasRules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "These results, indeed, make sense. The first rule is \"brilliant green\" -> \"iodine solution\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = fpm.associationRules\\\n",
    "            .filter(array_contains('antecedent', 2)) \\\n",
    "            .sort(desc('confidence'))\\\n",
    "            .toJSON()\\\n",
    "            .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[]'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'[{}]'.format(','.join(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exporting results to MongoDB\n",
    "The generated association rules will be used as lookup tables for our future HTTP server. Rules dataframe can be fetched directly, however, since it is hold in Spark context, it creates a significant overhead to find a specific row in the dataframe. Especially if we aim to create a relatively fast web-service. Therefore, we need to export the association rules out of Spark context. We could have stored it in another `json` file, but text files do not support random access, so there would not be any benefit. Additionally, in a real world case the file size might be enormous (i.e. for millions of rules). Thus, we need to store the rules in a database. For this task, I decided to use [MongoDB](http://api.mongodb.com/python/current/index.html), because it supports storing collections of millions of records in a distributed fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write the dataframe to MongoDB using mongo connector (uri specified in spark context configuration)\n",
    "rules.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Collaborative filtering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Matrix Factorization\n",
    "One of the most effective methods of collaborative filtering is Matrix Factorization. Matrix factorization is one of the algorithms from recommender systems family and as the name suggests it factorize a matrix, i.e., decompose a matrix in two(or more) matrices such that once you multiply them you get your original matrix back. In case of the recommendation system, we will typically start with an interaction/rating matrix between users and items and matrix factorization algorithm will decompose this matrix in user and item feature matrix which is also known as embeddings. Example of interaction matrix would be user-movie ratings for movie recommender, user-product purchase flag for transaction data, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![](https://cdn-images-1.medium.com/max/1600/0*Qy8Pku8FJXM60iAs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Typically user/item embeddings capture latent features about attributes of users and item respectively. Essentially, latent features are the representation of user/item in an arbitrary space which represents how a user rate a movie. In the example of a movie recommender, an example of user embedding might represent affinity of a user to watch serious kind of movie when the value of the latent feature is high and comedy type of movie when the value is low. Similarly, a movie latent feature may have a high value when the movie is more male driven and when it’s more female-driven the value is typically low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In case of product recomendations, the interaction matrix is very sparse and there are no explicit ratings. The approach used in spark.ml to deal with such data is taken from Collaborative Filtering for Implicit Feedback Datasets [12]. Essentially, instead of trying to model the matrix of ratings directly, this approach treats the data as numbers representing the strength in observations of user actions (such as the number of clicks, or the cumulative duration someone spent viewing a movie). Those numbers are then related to the level of confidence in observed user preferences, rather than explicit ratings given to items. The model then tries to find latent factors that can be used to predict the expected preference of a user for an item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this project, I used the total number of purchases for each item as an interaction measure. Alternating Least Squares algorithm implementation in `spark.ml` was used to calculate the latent factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# transform the transactions, aggregating total number of purchases per-item, per-user\n",
    "implicitRatings = df.select('id', \n",
    "                            # hash phone numbers to fit in Integer type\n",
    "                            (col('clientTelNumeric').cast(LongType()) * 67853 % 2**31).alias('telHashed'), \n",
    "                            explode('basket').alias('product'), \n",
    "                            'product.goodsId', \n",
    "                            'product.quantity') \\\n",
    "                    .groupBy('telHashed', 'goodsId') \\\n",
    "                    .agg(sum('quantity').alias('purchases')) \\\n",
    "                    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------+\n",
      "| telHashed|goodsId|purchases|\n",
      "+----------+-------+---------+\n",
      "|1121302175|  16588|        1|\n",
      "|1569021973| 116978|       30|\n",
      "|  21874890|  51767|        1|\n",
      "|  86765306|  26694|        1|\n",
      "|2022784980| 106619|        2|\n",
      "+----------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "implicitRatings.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.215298356790437"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# split into the train and test set\n",
    "training, test = implicitRatings.randomSplit([0.8, 0.2])\n",
    "\n",
    "# train an ALS model\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"telHashed\", itemCol=\"goodsId\", ratingCol=\"purchases\",\n",
    "          coldStartStrategy=\"drop\", implicitPrefs=True, nonnegative=True)\n",
    "model = als.fit(training)\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"purchases\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Fitting the best model is challenging and requires a lot of hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "One of the most effective use-cases of Matrix Factorization is finding similar items by distance between item embeddings. The item-based approach is proven to be more effective and flexible than other collaborative filtering techniques. It does not require retraining of the whole model for every new interaction and usually yields better results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Validation\n",
    "Validating a recommendation model with implicit recomedations is quite tricky. Basic RMSE does not fit in this case, because there is no information about movies that the user has rated negatively. Therefore, it is suggested to use recall-based metrics for such recommenders. Several metrics have been introduced, the most important being the Mean Percentage Ranking (MPR), also known as Percentile Ranking. However, there is no utility in Spark ML package to evaluate the model in such way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To serve the results over http, I decided to use [Flusk](https://github.com/pallets/flask) - a simple web-server framework for python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
